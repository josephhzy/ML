{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52152e52",
   "metadata": {},
   "source": [
    "# üßÆ Logistic Regression ‚Äî From First Principles\n",
    "\n",
    "*Implement it from scratch, understand every formula, and see why it is **classification**, not regression.*\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What Problem Does Logistic Regression Solve?\n",
    "\n",
    "|         | Regression models | **Logistic regression** |\n",
    "|---------|------------------|-------------------------|\n",
    "| **Output** | A **continuous** real number (e.g. house price) | A **probability** in *(0, 1)* ‚Üí hard‚Äêclass 0 / 1 |\n",
    "| **Typical question** | ‚ÄúHow much will the house cost?‚Äù | ‚ÄúWill the loan be **approved (1)** or **denied (0)**?‚Äù |\n",
    "| **Goal** | Minimise distance between predicted & true numbers | Maximise probability assigned to correct class |\n",
    "\n",
    "It is therefore a **binary classifier** even though the word *regression* lingers in its name.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Model Equation\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "z &= \\mathbf{w}^\\top\\mathbf{x} + b \\\\[4pt]\n",
    "\\hat{p} &= g(z) = \\frac{1}{1+e^{-z}} \\quad\\text{(sigmoid)} \\\\[4pt]\n",
    "\\underbrace{\\hat{y}}_{\\text{hard label}}\n",
    "&=\\begin{cases}\n",
    "1 &\\text{if }\\hat{p}\\ge 0.5\\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "![Sigmoid curve ‚Äî outputs stay between 0 and 1, crossing 0.5 when z = 0.](sigmoid.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "* **Linear score** \\(z\\) measures how far a point is from the decision boundary  \n",
    "* **Sigmoid** squashes any real \\(z\\) into a value strictly between 0 and 1  \n",
    "* **Decision boundary** is the hyper-plane where \\(z = 0\\) (because then \\(g(z)=0.5\\))\n",
    "- **w·µÄ (w-transpose)** ‚Äì the weight column-vector turned sideways into a row.  \n",
    "  *Purpose:* lets us compute the dot product `w·µÄ x = w1¬∑x1 + ‚Ä¶ + wk¬∑xk`, which equals the linear score z.\n",
    "\n",
    "- **≈∑_hard** (‚Äúhard hat-y‚Äù) ‚Äì the crisp 0 / 1 label after thresholding.  \n",
    "  *Rule:* if the probability p = g(z) is ‚â• 0.5, set ≈∑_hard = 1; otherwise 0.  \n",
    "  Needed for metrics like accuracy that expect exact class labels.\n",
    "\n",
    "- **Soft vs. hard outputs**  \n",
    "  - **Soft output:** p = g(z) ‚Äì a probability strictly between 0 and 1.  \n",
    "  - **Hard output:** ≈∑_hard ‚Äì that same probability converted to 0 or 1 by the threshold rule.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why Mean-Squared Error (MSE) **Breaks Down** for Logistic Regression  \n",
    "\n",
    "### 3.1 ‚Äî What if we na√Øvely reused MSE?\n",
    "\n",
    "$$\n",
    "C_{\\text{MSE}}(\\mathbf{w}, b)\n",
    "  = \\frac{1}{m} \\sum_{i=1}^{m} \\bigl(g(z_i) - y_i\\bigr)^2,\n",
    "\\qquad\n",
    "z_i = \\mathbf{w}^{\\top}\\mathbf{x}^{(i)} + b,\n",
    "\\qquad\n",
    "g(z) = \\frac{1}{\\,1 + e^{-z}}.\n",
    "$$\n",
    "\n",
    "### 3.2 ‚Äî The problem in pictures  \n",
    "\n",
    "| Linear regression | **Logistic + MSE** |\n",
    "|-------------------|--------------------|\n",
    "| ![smooth U-shape](linear.png) | ![many bumps](mse_bumpy.png) |\n",
    "| One clean, convex ‚Äúbowl‚Äù. | Plateaus at the edges, multiple little valleys (local minima). |\n",
    "\n",
    "### 3.3 ‚Äî Why do the bumps and plateaus appear with MSE?\n",
    "\n",
    "1. **Sigmoid saturation (flat tails)**  \n",
    "\n",
    "   $$\n",
    "   g(z)=\\frac{1}{1+e^{-z}},\n",
    "   \\qquad\n",
    "   g'(z)=g(z)\\bigl(1-g(z)\\bigr).\n",
    "   $$\n",
    "\n",
    "   * If $z\\ll 0$ ‚áí $g(z)\\approx 0$ and **$g'(z)\\approx 0$** ‚Äì the left tail is almost flat.  \n",
    "   * If $z\\gg 0$ ‚áí $g(z)\\approx 1$ and **$g'(z)\\approx 0$** ‚Äì the right tail is almost flat.  \n",
    "   These flat regions are the **plateaus** at the edges of the cost surface.\n",
    "   \n",
    "2. **What is $g$ and why is $g'(z)=g(z)\\,[1-g(z)]$ ?**\n",
    "\n",
    "- **Sigmoid definition**\n",
    "\n",
    "  $$\n",
    "  g(z)=\\frac{1}{\\,1+e^{-z}\\,}\n",
    "  $$\n",
    "\n",
    "- **Differentiate step-by-step**\n",
    "\n",
    "  1. Rewrite  \n",
    "\n",
    "     $$\n",
    "     g(z)=\\bigl(1+e^{-z}\\bigr)^{-1}\n",
    "     $$\n",
    "\n",
    "  2. Apply the chain rule  \n",
    "\n",
    "     $$\n",
    "     \\frac{d}{dz}\\bigl(u^{-1}\\bigr)=-u^{-2}\\,u',\n",
    "     \\qquad\n",
    "     u=1+e^{-z},\\; u'=-e^{-z}.\n",
    "     $$\n",
    "\n",
    "  3. Simplify to obtain  \n",
    "\n",
    "     $$\n",
    "     g'(z)=\\frac{e^{-z}}{\\bigl(1+e^{-z}\\bigr)^{2}}\n",
    "           =g(z)\\,\\bigl[1-g(z)\\bigr].\n",
    "     $$\n",
    "\n",
    "\n",
    "3. **Why does $x_j$ appear in the gradient $\\partial E/\\partial w_j$ ?**\n",
    "\n",
    "- **One-sample squared error**\n",
    "\n",
    "  $$\n",
    "  E=\\bigl(g(z)-y\\bigr)^{2},\n",
    "  \\qquad\n",
    "  z=\\mathbf{w}^{\\top}\\mathbf{x}+b.\n",
    "  $$\n",
    "\n",
    "- **Chain-rule expansion**\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial E}{\\partial w_j}\n",
    "    =2\\bigl(g(z)-y\\bigr)\\,\n",
    "      g'(z)\\,\n",
    "      \\underbrace{\\frac{\\partial z}{\\partial w_j}}_{=\\,x_j}.\n",
    "  $$\n",
    "\n",
    "- **Final form**\n",
    "\n",
    "  $$\n",
    "  \\boxed{\\displaystyle\n",
    "  \\frac{\\partial E}{\\partial w_j}\n",
    "     =2\\bigl(g(z)-y\\bigr)\\,g'(z)\\,x_j}\n",
    "  $$\n",
    "\n",
    "  The feature value $x_j$ appears because changing weight $w_j$ shifts the linear score $z$ by exactly $x_j$; larger features exert a proportionally larger influence on the gradient.\n",
    "  \n",
    "4. **Squared error sees only distance, not direction**  \n",
    "\n",
    "   For one sample  \n",
    "   $$\n",
    "   E \\;=\\; \\bigl(g(z)-y\\bigr)^2.\n",
    "   $$  \n",
    "\n",
    "   Gradient w.r.t. a weight $w_j$:  \n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial E}{\\partial w_j}\n",
    "      = 2\\bigl(g(z)-y\\bigr)\\;g'(z)\\;x_j.\n",
    "   $$  \n",
    "\n",
    "   *On the plateaus* $g'(z)\\!\\approx\\!0$, so the gradient almost vanishes even if the prediction is wrong ‚Äî optimisation ‚Äústalls‚Äù on those flats.\n",
    "\n",
    "5. **Opposing pulls from labels 0 and 1 create ripples**  \n",
    "\n",
    "   * A point with $y=0$ wants $g(z)\\!\\downarrow$ (push $z$ left).  \n",
    "   * A point with $y=1$ wants $g(z)\\!\\uparrow$ (push $z$ right).  \n",
    "\n",
    "   Each class digs its own shallow valley in a different region.  \n",
    "   Summing all those valleys produces the **multi-hump landscape** (many local minima).\n",
    "\n",
    "> **Result:** Gradient descent can settle in a **shallow local dip** and stop, never reaching the true global minimum.\n",
    "\n",
    "---\n",
    "\n",
    "## 4 ¬∑ Log-Loss (Cross-Entropy) ‚Äî How It ‚ÄúIrons the Plateaus Flat‚Äù\n",
    "\n",
    "For **each** training example the loss is  \n",
    "\n",
    "$$\n",
    "L(\\hat{p},y)=\n",
    "\\begin{cases}\n",
    "-\\log(\\hat{p})            & \\text{if } y = 1,\\\\[6pt]\n",
    "-\\log\\!\\bigl(1-\\hat{p}\\bigr) & \\text{if } y = 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* **$-\\log(\\hat{p})$** shoots upward as $\\hat{p}\\!\\to 0$ &nbsp;‚Üí&nbsp; harshly punishes *confident-wrong* predictions for class 1.  \n",
    "* **$-\\log(1-\\hat{p})$** shoots upward as $\\hat{p}\\!\\to 1$ &nbsp;‚Üí&nbsp; harshly punishes *confident-wrong* predictions for class 0.  \n",
    "\n",
    "\n",
    "| Loss curve for **$y = $**  (`- log(p)`) | Loss curve for **$y = $**  (`- log(1 ‚Äì p)`) |\n",
    "|------------------------------------------|----------------------------------------------|\n",
    "| ![‚àílog(p)](-logx.png) | ![‚àílog(1 ‚àí p)](-log1-x.png) |\n",
    "\n",
    "\n",
    "These logarithmic ‚Äúwalls‚Äù keep the gradient alive even on the sigmoid‚Äôs flat tails, so the many little valleys produced by MSE are **merged into one deep, convex bowl**.\n",
    "\n",
    "\n",
    "### Average loss over the dataset  \n",
    "\n",
    "$$\n",
    "C(\\mathbf{w},b)=\n",
    "\\frac{1}{m}\\sum_{i=1}^{m}\n",
    "\\Bigl[-\\,y_i\\,\\log\\hat{p}_i\\;-\\;(1-y_i)\\,\\log\\bigl(1-\\hat{p}_i\\bigr)\\Bigr],\n",
    "\\qquad\n",
    "\\hat{p}_i = g\\!\\bigl(\\mathbf{w}^{\\top}\\mathbf{x}^{(i)}+b\\bigr).\n",
    "$$\n",
    "\n",
    "* **Correct & confident** ‚ü∂ loss $\\to 0$  \n",
    "* **Wrong & confident** ‚ü∂ loss $\\to \\infty$  \n",
    "* The summed surface is always **smooth & convex** ‚Üí gradient descent finds the single global minimum.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5 ¬∑ Gradient Descent ‚Äî Fitting $w$ and $b$\n",
    "\n",
    "> Iteratively slide **downhill** on the log-loss surface until you reach the single global bowl‚Äôs bottom.\n",
    "\n",
    "### 5.1 Compute the gradients\n",
    "\n",
    "For every training sample \\(i\\):\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "z_i &= \\mathbf{w}^{\\top}\\mathbf{x}^{(i)} + b \\\\[4pt]\n",
    "\\hat{p}_i &= g(z_i)=\\frac{1}{1+e^{-z_i}}\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "Using the log-loss cost derived in Section&nbsp;4,\n",
    "\n",
    "$$\n",
    "C(\\mathbf{w},b)\n",
    "   =\\frac{1}{m}\\sum_{i=1}^{m}\n",
    "     \\Bigl[-\\,y_i\\,\\log(\\hat{p}_i)\n",
    "           \\;-\\;(1-y_i)\\,\\log\\!\\bigl(1-\\hat{p}_i\\bigr)\n",
    "     \\Bigr],\n",
    "\\qquad\n",
    "\\hat{p}_i = g\\!\\bigl(\\mathbf{w}^{\\top}\\mathbf{x}^{(i)} + b\\bigr).\n",
    "$$\n",
    "\n",
    "**Partial derivatives**\n",
    "\n",
    "$$\n",
    "\\boxed{\\;\n",
    "\\frac{\\partial C}{\\partial w_j}\n",
    "   = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{p}_i - y_i)\\,x_{i,j}\n",
    "\\;}\n",
    "\\qquad\n",
    "\\boxed{\\;\n",
    "\\frac{\\partial C}{\\partial b}\n",
    "   = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{p}_i - y_i)\n",
    "\\;}\n",
    "$$\n",
    "\n",
    "\n",
    "*Reasoning:*  \n",
    "\n",
    "$$\n",
    "g'(z)=g(z)\\,[1-g(z)]\n",
    "\\qquad\\text{and}\\qquad\n",
    "\\frac{\\partial z_i}{\\partial w_j}=x_{i,j}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc4c24",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
